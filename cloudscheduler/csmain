#!/usr/bin/python3

import os
import sys
import json
import math
import time
import logging
import signal
import socket
import multiprocessing
from multiprocessing import Process
from collections import defaultdict

from cloudscheduler.lib.db_config import Config
from cloudscheduler.lib.view_utils import qt, retire_cloud_vms
from cloudscheduler.lib.log_tools import get_frame_info
from cloudscheduler.lib.ec2_translations import get_ami_dictionary
from cloudscheduler.lib.ProcessMonitor import ProcessMonitor, check_pid, terminate
from cloudscheduler.lib.poller_functions import start_cycle, wait_cycle
from cloudscheduler.lib.openstack_functions import get_openstack_sess, get_nova_connection, get_cinder_connection

import openstackcloud
import localhostcloud
import ec2cloud

import htcondor
import classad
import boto3

import openstack


### DB PERMISSIONS STUFF
#    .db_query('view_available_resources')

def _get_ec2_session(cloud):
    return boto3.session.Session(region_name=cloud["region"],
                                 aws_access_key_id=cloud["username"],
                                 aws_secret_access_key=cloud["password"])

def _get_ec2_client(session):
    return session.client('ec2')


def get_condor_session(hostname=None):
    try:
        condor_session = htcondor.Collector(hostname)
        return condor_session
    except Exception as exc:
        logging.exception("Failed to get condor session for %s:" % hostname)
        logging.error(exc)
        return False


def get_master_classad(session, machine, hostname):
    MASTER_TYPE = htcondor.AdTypes.Master
    STARTD_TYPE = htcondor.AdTypes.Startd
    try:
        if machine is not "":
            condor_classad = session.query(MASTER_TYPE, 'Name=="%s"' % machine)[0]
        else:
            condor_classad = session.query(MASTER_TYPE, 'regexp("%s", Name, "i")' % hostname)[0]
        return condor_classad
    except IndexError:
        logging.error("Failed to retrieve classad from condor. No matching classad")
    except Exception as exc:
        logging.error("Failed to retrieve classad from condor. Communication error :")
        logging.error(exc)
        return -1
    return False


def get_startd_classads(session, machine):
    startd_list = []
    STARTD_TYPE = htcondor.AdTypes.Startd
    try:
        condor_classads = session.query(STARTD_TYPE, 'Machine=="%s"' % machine)
        for classad in condor_classads:
            startd_list.append(classad)
        return startd_list
    except Exception as exc:
        logging.error("Failed to retrieve machine classads, aborting...")
        logging.error(exc)
        return False


def invalidate_master_classad(session, classad):
    return session.advertise([classad], "INVALIDATE_MASTER_ADS")

def invalidate_startd_classads(session, classad_list):
    return session.advertise(classad_list, "INVALIDATE_STARTD_ADS")

def available_resources_initialize(config):
    used_resource_list = []
    rc, msg = config.db_execute('select * from view_total_used_resources')
    for row in config.db_cursor:
        used_resource_list.append(row)

    used_resources = {'group_cloud': {}, 'provider': {}}
    for used_resource in used_resource_list:
        provider = '%s|%s|%s' % (used_resource['authurl'], used_resource['region'], used_resource['project'])
        if provider not in used_resources['provider']:
            used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

        used_resources['provider'][provider]['VMs']        += used_resource['VMs']
        used_resources['provider'][provider]['cores_used'] += used_resource['cores']
        used_resources['provider'][provider]['disk_used']  += used_resource['disk']
        used_resources['provider'][provider]['ram_used']   += used_resource['ram']
        used_resources['provider'][provider]['swap_used']  += used_resource['swap']

    for provider in sorted(used_resources['provider']):
        log.debug("available_resources_initialize, VMs: %11s, cores: %11s, disk: %11s, RAM: %11s, provider: %s" % (
            used_resources['provider'][provider]['VMs'],
            used_resources['provider'][provider]['cores_used'],
            used_resources['provider'][provider]['disk_used'],
            used_resources['provider'][provider]['ram_used'],
            provider
            ))

    return used_resources

def available_resources_query(used_resources, available_resource):
    def set_default(ctl, default_value):
        if ctl < 0:
            return default_value
        else:
            return ctl

    group_cloud = '%s::%s' % (available_resource['group_name'], available_resource['cloud_name'])
    if group_cloud not in used_resources['group_cloud']:
        used_resources['group_cloud'][group_cloud] = {
            'VMs': available_resource['VMs'],
            'cores_used': available_resource['cores_used'],
            'disk_used': available_resource['disk_used'],
            'ram_used': available_resource['ram_used'],
            'swap_used': available_resource['swap_used'],
            }

    provider = '%s|%s|%s' % (available_resource['authurl'], available_resource['region'], available_resource['project'])
    if provider not in used_resources['provider']:
        used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

    cores_limit = min(
        set_default(available_resource['cores_ctl'], available_resource['cores_max']),
        set_default(available_resource['cores_softmax'], available_resource['cores_max']) - max(0, used_resources['provider'][provider]['cores_used'] - used_resources['group_cloud'][group_cloud]['cores_used']),
        available_resource['cores_max'] - max(0, used_resources['provider'][provider]['cores_used'] - used_resources['group_cloud'][group_cloud]['cores_used'])
        )

    ram_limit = min(
        set_default(available_resource['ram_ctl'], available_resource['ram_max']),
        available_resource['ram_max'] - max(0, used_resources['provider'][provider]['ram_used'] - used_resources['group_cloud'][group_cloud]['ram_used'])
        )

    log.debug("provider: %s used:%s" % (used_resources['provider'][provider]['VMs'], used_resources['group_cloud'][group_cloud]['VMs'])) 

    slots = min(
        available_resource['VMs_max'] - used_resources['provider'][provider]['VMs'],
        int(max(0, cores_limit - used_resources['group_cloud'][group_cloud]['cores_used']) / available_resource['flavor_cores']),
        int(max(0, ram_limit - used_resources['group_cloud'][group_cloud]['ram_used']) / available_resource['flavor_ram'])
        )

    log.debug("available_resources_query(%s), VMs(%s/%s), Cores(%s,%s), RAM(%s,%s) Slots(%s) available for resource: %s" % (
        available_resource['group_name'],
        used_resources['group_cloud'][group_cloud]['VMs'],
        used_resources['provider'][provider]['VMs'],
        used_resources['group_cloud'][group_cloud]['cores_used'],
        cores_limit,
        used_resources['group_cloud'][group_cloud]['ram_used'],
        ram_limit,
        slots,
        available_resource['flavor']
        ))
    return slots

def available_resources_update(used_resources, available_resource, consumed_slots):
    consumed_cores = available_resource['flavor_cores'] * consumed_slots
    consumed_ram = available_resource['flavor_ram'] * consumed_slots

    group_cloud = '%s::%s' % (available_resource['group_name'], available_resource['cloud_name'])
    if group_cloud not in used_resources['group_cloud']:
        used_resources['group_cloud'][group_cloud] = {
            'VMs': available_resource['VMs'],
            'cores_used': available_resource['cores_used'],
            'disk_used': available_resource['disk_used'],
            'ram_used': available_resource['ram_used'],
            'swap_used': available_resource['swap_used'],
            }

    provider = '%s|%s|%s' % (available_resource['authurl'], available_resource['region'], available_resource['project'])
    if provider not in used_resources['provider']:
        used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

    pre_group_VMs = used_resources['group_cloud'][group_cloud]['VMs']
    pre_group_cores = used_resources['group_cloud'][group_cloud]['cores_used']
    pre_group_ram = used_resources['group_cloud'][group_cloud]['ram_used']

    used_resources['group_cloud'][group_cloud]['VMs'] += consumed_slots
    used_resources['group_cloud'][group_cloud]['cores_used'] += consumed_cores
    used_resources['group_cloud'][group_cloud]['ram_used'] += consumed_ram

    pre_provider_VMs = used_resources['provider'][provider]['VMs']
    pre_provider_cores = used_resources['provider'][provider]['cores_used']
    pre_provider_ram = used_resources['provider'][provider]['ram_used']

    used_resources['provider'][provider]['VMs'] += consumed_slots
    used_resources['provider'][provider]['cores_used'] += consumed_cores
    used_resources['provider'][provider]['ram_used'] += consumed_ram

    log.debug("available_resources_update(%s, %s), VMs(%s/%s, %s/%s), Cores(%s/%s, %s/%s), RAM(%s/%s, %s/%s) Slots(%s)" % (
        available_resource['group_name'],
        available_resource['flavor'],

        pre_group_VMs,
        used_resources['group_cloud'][group_cloud]['VMs'],
        pre_provider_VMs,
        used_resources['provider'][provider]['VMs'],

        pre_group_cores,
        used_resources['group_cloud'][group_cloud]['cores_used'],
        pre_provider_cores,
        used_resources['provider'][provider]['cores_used'],

        pre_group_ram,
        used_resources['group_cloud'][group_cloud]['ram_used'],
        pre_provider_ram,
        used_resources['provider'][provider]['ram_used'],

        consumed_slots,
        ))

def check_view_idle_vms():
    """Query view_idle_vms and retire as needed."""
    multiprocessing.current_process().name = "csmain_idle_vms"
    log = logging.getLogger(__name__)
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', 'csmain',
                    pool_size=15)
    if not config:
        print("Problem loading config file.")
        return
    while True:
        config.db_open()
        config.refresh()
        log.debug("-------------------check_view_idle_vms-------------------")
        try:
            where_clause = "retire=0 and terminate=0"
            rc, msg, results = config.db_query("view_idle_vms", where=where_clause)
            vmids = []
            for res in results:
                vmids.append(res["vmid"])
            if vmids:
                log.debug("Setting retire flag on %s VMs.", len(vmids))
                vms = "csv2_vms"
                where_clause = "vmid in ("
                for id in vmids:
                    where_clause += "'%s'," % id
                #replace last , with closing bracket
                where_clause = where_clause[:-1] + ")"
                rc, msg, update_result = config.db_query(vms, where=where_clause)
                for row in update_result:
                    row["retire"] = 1
                    old_updater = row["updater"]
                    row["updater"] = get_frame_info() + ":r1"
                    config.db_merge(vms, row)
                    log.debug("Set retire flag on %s, previous updater: %s",
                              row["hostname"], old_updater)
            config.db_close(commit=True)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
        time.sleep(config.categories['csmain']['sleep_interval_main_long'])


def _exit(signal_num, frame):
    logging.info("Got signal: %s, exiting.." % signal_num)
    exit(0)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def process_group_cloud_terminates(pair, config):
    group_name = pair["group_name"]
    cloud_name = pair["cloud_name"]

    #config.db_open()

    VM = "csv2_vms"
    CLOUD = "csv2_clouds"

    terminate_off = config.categories[os.path.basename(sys.argv[0])]["terminate_off"]

    master_type = htcondor.AdTypes.Master
    startd_type = htcondor.AdTypes.Startd

    logging.debug("Processing commands for group:%s, cloud:%s" % (group_name, cloud_name))

    # Terminate code:
    master_list = []
    startd_list = []
    where_clause = "cloud_name='%s' and group_name='%s' and terminate>=1" % (cloud_name, group_name)
    rc, msg, redundant_machine_list = config.db_query("view_condor_host", where=where_clause)
    logging.debug("%s : %s: Actionable VMS:" % (group_name, cloud_name))
    for resource in redundant_machine_list:
        if resource["dynamic_slots"] is not None:
            if resource["primary_slots"] is not None:
                if resource["terminate"] == 1 and resource["dynamic_slots"] >= 1 and resource["primary_slots"] >=1:
                    logging.info("%s : %s: VM still has active slots, skipping terminate on %s" % (group_name, cloud_name, resource["vmid"]))
                    continue


        # we need the relevent vm row to check if its in manual mode and if not, terminate and update termination status
        try:
            where_clause = "group_name='%s' and cloud_name='%s' and vmid='%s'" % (resource["group_name"], resource["cloud_name"], resource["vmid"])
            rc, msg, vm_rows = config.db_query(VM, where=where_clause)
            vm_row = vm_rows[0]
        except Exception as exc:
            logging.error("%s : %s: Unable to retrieve VM row for vmid: %s, skipping terminate..." % (group_name, cloud_name, resource["vmid"]))
            continue
        if vm_row["manual_control"] == 1:
            logging.info("%s : %s: VM %s under manual control, skipping terminate..." % (group_name, cloud_name, resource["vmid"]))
            continue


        # Get session with hosting cloud.
        where_clause = "group_name='%s' and cloud_name='%s'" % (vm_row["group_name"], vm_row["cloud_name"])
        rc, msg, clouds = config.db_query(CLOUD, where=where_clause)
        cloud=clouds[0]

        if cloud["cloud_type"] == "openstack":
            session = get_openstack_sess(cloud, config.categories["condor_poller.py"]["cacerts"])
            if session is False:
                continue
         
            if terminate_off:
                logging.critical("%s : %s: Terminates disabled, normal operation would terminate %s" % (group_name, cloud_name, vm_row["hostname"]))
                continue

            # terminate the vm
            nova = get_nova_connection(session, region=cloud["region"])

            if nova is False:
                logging.error("Openstack nova connection failed")
                continue

            try:
                # may want to check for result here Returns: An instance of novaclient.base.TupleWithMeta so probably not that useful
                vm_row["terminate"] = vm_row["terminate"] + 1
                old_updater = vm_row["updater"]
                vm_row["updater"] = str(get_frame_info() + ":t+")

                try:
                    nova.delete_server(vm_row["vmid"], ignore_missing=False)
                except openstack.exceptions.ResourceNotFound as ex:
                    logging.error("%s : %s: VM not found on cloud, deleting vm entry %s" % (group_name, cloud_name, vm_row["vmid"]))
                    config.db_delete(VM, vm_row)
                    try:
                        config.db_commit()
                    except Exception as exc:
                        logging.error("%s : %s: Failed to commit vm delete, aborting cycle..." % (group_name, cloud_name))
                        logging.exception(exc)
                        break
                    continue #skip rest of logic since this vm is gone anywheres 
                        

                except Exception as exc:
                    logging.error("%s : %s: Unable to delete vm, vm doesnt exist or openstack failure:" % (group_name, cloud_name))
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    logging.error(exc_type)
                    logging.error(exc)
                    continue
                logging.info("%s : %s: VM Terminated(%s): %s primary slots: %s dynamic slots: %s, last updater: %s" % (group_name, cloud_name, vm_row["terminate"], vm_row["hostname"], vm_row["htcondor_partitionable_slots"], vm_row["htcondor_dynamic_slots"], old_updater))
                config.db_merge(VM, vm_row)
                # log here if terminate # /10 = remainder zero
                if vm_row["terminate"] %10 == 0:
                    logging.critical("%s : %s: %s failed terminates on %s user action required" % (group_name, cloud_name, vm_row["terminate"] - 1, vm_row["hostname"]))
            except Exception as exc:
                logging.error("%s : %s: Failed to terminate VM: %s, terminates issued: %s" % (group_name, cloud_name, vm_row["hostname"], vm_row["terminate"] - 1))
                logging.error(exc)

            # Now that the machine is terminated, we can speed up operations by invalidating the related classads
            # double check that a condor_session exists
            if 'condor_session' not in locals():
                condor_session = get_condor_session()
            if resource["machine"] is not None:
                logging.info("%s : %s: Removing classads for machine %s" % (group_name, cloud_name, resource["machine"]))
            else:
                logging.info("%s : %s: Removing classads for machine %s" % (group_name, cloud_name, resource["hostname"]))
            try:
                master_classad = get_master_classad(condor_session, resource["machine"], resource["hostname"])
                if not master_classad:
                    # there was no matching classad
                    logging.error("%s : %s: Classad not found for %s//%s" % (group_name, cloud_name, resource["machine"], resource["hostname"]))

                master_result = invalidate_master_classad(condor_session, master_classad)
                logging.debug("%s : %s: Invalidate master result: %s" % (group_name, cloud_name, master_result))
                startd_classads = get_startd_classads(condor_session, resource["hostname"])
                startd_result = invalidate_startd_classads(condor_session, startd_classads)
                logging.debug("%s % %s: Invalidate startd result: %s" % (group_name, cloud_name, startd_result))


                #if resource.machine is not None and resource.machine is not "":
                #    condor_classad = condor_session.query(master_type, 'Name=="%s"' % resource["machine"])[0]
                #else:
                #    condor_classad = condor_session.query(master_type, 'regexp("%s", Name, "i")' % resource["hostname"])[0]
                #master_list.append(condor_classad)

                # this could be a list of adds if a machine has many slots
                #condor_classads = condor_session.query(startd_type, 'Machine=="%s"' % resource["hostname"])
                #for classad in condor_classads:
                #    startd_list.append(classad)
            #except IndexError as exc:
            #    pass
            except Exception as exc:
                logging.error("%s : %s: Failed to get condor classads or issue invalidate:" % (group_name, cloud_name))
                logging.error(exc)
                continue

        elif cloud["cloud_type"] == "amazon":
            if terminate_off:
                logging.critical("%s : %s: Terminates disabled, normal operation would terminate %s" % (group_name, cloud_name, vm_row["hostname"]))
                continue
            #terminate the vm
            amz_session = _get_ec2_session(cloud)
            amz_client = _get_ec2_client(amz_session)
            try:
                vm_row.terminate = vm_row["terminate"] + 1
                old_updater = vm_row["updater"]
                vm_row["updater"] = str(get_frame_info() + ":t+")
                #destroy amazon vm, first we'll need to check if its a reservation
                if vm_row["vmid"][0:3].lower() == "sir":
                    #its a reservation just delete it and destroy the vm
                    # not sure what the difference between a client and connection from csv1 is but there is more work to be done here
                    #
                    # need the command to remove reservation:
                    # cancel_spot_instance_requests(list_of_request_ids)
                    #
                    # need to terminate request, and possible image if instance_id isn't empty
                    try:
                        logging.info("%s %s: Canceling amazon spot price request: %s" % (group_name, cloud_name, vm_row["vmid"]))
                        amz_client.cancel_spot_instance_requests(SpotInstanceRequestIds=[vm_row["vmid"]])
                        if vm_row["instance_id"] is not None:
                            #spot price vm running need to terminate it:
                            logging.info("%s : %s: Terminating amazon vm: %s" % (group_name, cloud_name, vm_row["instance_id"]))
                            amz_client.terminate_instances(InstanceIds=[vm_row["instance_id"]])
                    except Exception as exc:
                        logging.error("%s : %s: Unable to terminate %s due to:" % (group_name, cloud_name, vm_row["vmid"]))
                        logging.error(exc)
                else:
                    #its a regular instance and just terminate it
                    logging.info("%s : %s: Terminating amazon vm: %s" % (group_name, cloud_name, vm_row["vmid"]))
                    amz_client.terminate_instances(InstanceIds=[vm_row["vmid"]])
            except Exception as exc:
                logging.error("%s : %s: Unable to terminate %s due to:" % (group_name, cloud_name, vm_row["vmid"]))
                logging.error(exc)


        else:
            # Other cloud types will need to be implemented here to terminate any vms not from openstack
            logging.info("%s : %s: Vm not from openstack, or amazon cloud, skipping..." % (group_name, cloud_name))
            continue
    try:
        config.db_commit()
    except Exception as exc:
        logging.exception("%s : %s: Failed to commit vm updates, aborting cycle..." % (group_name, cloud_name))
        logging.error(exc)
        return

    logging.debug("%s : %s: Commands complete" % (group_name, cloud_name))
    return


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Main functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def main():
    """
    main function.
    """
    multiprocessing.current_process().name = "csmain"

    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', ['csmain', 'GSI'], pool_size=25)
    if not config:
        print("Problem loading config file.")
    log = logging.getLogger(__name__)

    cs_host = socket.gethostname()
    try:
        cs_host_ip = socket.gethostbyname(cs_host)
    except:
        cs_host_ip = '*** unresolved ***'

    while_counter = 0
    while(True):
        config.db_open()
        config.refresh()
        logging.basicConfig(filename=config.categories['csmain']['log_file'],
                            level=config.categories['csmain']['log_level'],
                            format="%(asctime)s - %(processName)-12s - %(levelname)s - %(message)s")
        
        # check/disable if any cloud will expire in one week
        disable_expired_clouds(config)

        # test remove_orphaned_volumes
        remove_orphaned_volumes(config)

        # Get the metadata for groups as json selects with mime_types pre-sorted
        # metadata format: { group : { cloud: [(yaml select, mime_type), (yaml select, mime_type) ] } }
        try:
            ec2_image_dict = get_ami_dictionary()
        except Exception as ex:
            log.exception(ex)
            continue

        try:
            rc, msg, g_metadatas = config.db_query("view_metadata_collation_json")
            metadata = json.loads(g_metadatas[0]["group_metadata"])
        except AttributeError as ae:
            log.exception("Problem loading metadata, is there no metadata? %s" % ae)
            metadata = {}
        except Exception as exc:
            log.debug("Problem loading metadata, no metadata %s" % exc)
            metadata = {}

        used_resources = available_resources_initialize(config)

        rc, msg, avail_resources = config.db_query("view_available_resources")
        available_resources_dict = qt(avail_resources,
            keys = {
                'primary': [
                    'group_name',
                    'flavor',
                ]
            }
        )

        try:
            # Booting up new VMs to fill in any free space on available clouds related to idle queued jobs
            # Get the idle jobs for the current group

            # need to transform this into a tiered dictionary keyed on group
            rc, msg, grps_of_idle_jobs = config.db_query("view_groups_of_idle_jobs", order_by="job_priority")
            idle_job_groups = qt(grps_of_idle_jobs,
                 keys = {
                    'primary': [
                        'group_name',
                    ],
                    'list_duplicates': True,
                }
            )
            list_groups_with_idle_jobs = sorted(idle_job_groups.keys())
            list_groups_with_idle_jobs_len = len(list_groups_with_idle_jobs)
            if list_groups_with_idle_jobs_len > 0:
                list_groups_with_idle_jobs_ix = while_counter % list_groups_with_idle_jobs_len
            else:
                list_groups_with_idle_jobs_ix = 0
            
            log.info('while_counter: %s, groups with idle jobs (%s/%s): %s' % (
                while_counter,
                list_groups_with_idle_jobs_ix,
                list_groups_with_idle_jobs_len,
                list_groups_with_idle_jobs[list_groups_with_idle_jobs_ix:] + list_groups_with_idle_jobs[:list_groups_with_idle_jobs_ix],
                ))

#           cloud_booted_on = set()  #  Setting this up here now so I can check it and skip over things I boot on due to available slots info will be wrong
            cloud_prio = None 
            if list_groups_with_idle_jobs_len > 0:
                for job_group in list_groups_with_idle_jobs[list_groups_with_idle_jobs_ix:] + list_groups_with_idle_jobs[:list_groups_with_idle_jobs_ix]:
#               for job_group in idle_job_groups:
                    ####### log.debug("Proccessing group: %s" % job_group)
                    booted_for_group = False
                    
                    # This old for loop does not share between aliases, it has been updated to share between the job_groups using the same method as groups
                    #for idle_job in idle_job_groups[job_group]:
                    num_job_groups = len(idle_job_groups[job_group])
                    if num_job_groups > 0:
                        job_grp_modulo =  while_counter % num_job_groups
                    else:
                        job_grp_modulo = 0
                    for idle_job in idle_job_groups[job_group][job_grp_modulo:] + idle_job_groups[job_group][:job_grp_modulo]:
                        current_boot_target_alias = idle_job.get("target_alias")
                        ####### log.debug("Idle jobs for group: %s" % idle_job_groups[job_group])
                        if idle_job.get("idle") == 0:
                            continue
                        if booted_for_group:
                            break # this may change in the future once we have smarter booting


                        ####### log.debug("Info for current job: Group: %s, Target: %s, User: %s, Flavors: %s",
                        # flavors is generated line based on which flavor is the best fit and which clouds have available resources
                        # the format is "Group:Cloud:Flavor, Group:Cloud:Flavor" will need to split and use to filter in resources_matching

                        if idle_job.get("flavors"):
                            flavor_list = [x.strip() for x in idle_job.get("flavors").split(',')]
                        else:
                            continue

                        boot_flavor = None
                        for flavor in flavor_list:
                            if available_resources_dict[job_group].get(flavor, None) is not None:
                                #Found flavour with available resources
                                ####### log.debug("Flavor:%s found to have available slots" % flavor)
                                log.info("Flavor:%s found to have available slots" % flavor)
                                boot_flavor = available_resources_dict[job_group].get(flavor, None)
                            else:
                                ####### log.debug("No available slots for flavor: %s, trying next..." % flavor)
                                continue

                            # We have a boot flavor, check that there is available resources and check shortfalls
                            # If we continue here we go on to the next flavor until we run out
                            # boot_flavor = available_resource row with all info that we need to boot a vm

                            available_slots = available_resources_query(used_resources, boot_flavor)
    #                       if available_slots <= 0:
                                ####### log.debug("No available resources to boot %s" % boot_flavor.get("flavor"))

                            try:
                                if idle_job.get("target_alias") is not None: 
                                    where_clause = "group_name='%s' and target_alias='%s'" % (idle_job.get("group_name"), idle_job.get("target_alias"))
                                else:
                                    where_clause = "group_name='%s' and target_alias is NULL" % idle_job.get("group_name")
                                rc, msg, shortfall = config.db_query("view_active_resource_shortfall", where=where_clause)
                            except:
                                shortfall = []


                            if len(shortfall) < 1:
                                log.warning('unable to retrieve view_active_resource_shortfall for group=%s, target_alias=%s, trying next flavor or skiping idle_job_group).',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"))
                                continue

                            if shortfall[0]["shortfall_cores"]<=0 and shortfall[0]["shortfall_disk"]<=0 and shortfall[0]["shortfall_ram"]<=0:
                                log.info('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, no shortfall, trying next flavor or skipping idle_job_group.',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["shortfall_cores"],
                                    shortfall[0]["shortfall_disk"],
                                    shortfall[0]["shortfall_ram"])
                                continue
                                
                            idle_VMs_throttle = max(config.categories['csmain']['idle_VMs_throttle'], int(shortfall[0]["running"]/10))
                            if shortfall[0]["idle"]>=idle_VMs_throttle:
                                log.info('Too many idle VMs: group=%s, target_alias=%s, idle=%s, running=%s, calculated throttle=%s configurred throttle=%s, trying next flavor or skipping idle_job_group.',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["idle"],
                                    shortfall[0]["running"],
                                    idle_VMs_throttle,
                                    config.categories['csmain']['idle_VMs_throttle'])
                                continue

                            else:
                                ##log.debug('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, too few active resources, processing continues...',
                                log.info('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, too few active resources, processing continues...',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["shortfall_cores"],
                                    shortfall[0]["shortfall_disk"],
                                    shortfall[0]["shortfall_ram"])

                                # Looks like we need more VMs, we should have the flavor slots available from the view_available resources row.
                                # Before we boot anything lets check resrouce contention
                                try:
                                    where_clause = "authurl='%s'" % boot_flavor.get("authurl")
                                    rc, msg, resource = config.db_query("view_resource_contention", where=where_clause)
                                except:
                                     resource = []

                                if len(resource) < 1:
                                    resource = [{
                                        "authurl":boot_flavor.get("authurl"),
                                        "VMs": 0,
                                        "starting": 0,
                                        "unregistered": 0,
                                        "idle": 0,
                                        "running": 0,
                                        "retiring": 0,
                                        "manual": 0,
                                        "error": 0
                                    }]

                                    log.debug('No active vms for group=%s, cloud=%s, authurl=%s, assuming no contention).',
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("cloud_name"),
                                        boot_flavor.get("authurl"))

                                if resource[0]["starting"] + resource[0]["unregistered"] >= config.categories['csmain']['new_VMs_throttle']:
                                    log.info('Resource contention: group=%s, cloud=%s, resource=%s, starting=%s, unregistered=%s, trying next flavor or moving on to next job group.',
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("cloud_name"),
                                        resource[0]["authurl"],
                                        resource[0]["starting"],
                                        resource[0]["unregistered"])
                                    continue

#                               if idle_job.get("cloud_name") in cloud_booted_on:
#                                   log.debug("Already booted VMs on %s, skipping to next resource..." % boot_flavor.get("cloud_name"))
#                                   continue

                                # Set cloud priority so we can be sure to fill clouds of this priority first.
                                if cloud_prio == None:
                                    cloud_prio = boot_flavor.get("cloud_priority ")# Set the lowest priority
                                elif cloud_prio == boot_flavor.get("cloud_priority"):
                                    pass # keep booting VMs on clouds of the same priority if possible
                                else:
                                    log.info("Cloud: %s has low priority, skipping until high priority clouds full.." % boot_flavor.get("cloud_name"))
                                    continue # if higher priority cloud found skip

                                current_group_name = idle_job.get("group_name")
                                current_boot_cloud = boot_flavor.get("cloud_name")
#                               current_boot_target_alias = idle_job.get("target_alias")
                                current_boot_flavor = boot_flavor.get("flavor")
                                ####### log.debug("Taking a look at booting on: %s, using flavor: %s" % (current_boot_cloud, current_boot_flavor))
                                def no_zero(val):
                                    if int(val) > 0:
                                        return int(val)
                                    else:
                                        return 1
                                shortfall_cores = math.ceil(shortfall[0]["shortfall_cores"]/no_zero(boot_flavor['flavor_cores']))
                                shortfall_disk = math.ceil(shortfall[0]["shortfall_disk"]/no_zero(boot_flavor['flavor_disk']))
                                shortfall_ram = math.ceil(shortfall[0]["shortfall_ram"]/no_zero(boot_flavor['flavor_ram']))
                                shortfall_slots = max(0,
                                    shortfall_cores,
                                    shortfall_disk,
                                    shortfall_ram
                                    )
                                
                                num_vms_to_boot = min(
                                    config.categories['csmain']['max_start_vm_cloud'],
                                    available_slots,
                                    shortfall_slots
                                    )
                                if num_vms_to_boot <= 0:
                                    cloud_prio = None # reset cloud prio so if no slots on the higher priority clouds can still boot on lower
                                    log.debug("No Flavor Slots for %s %s, minumum of max_start_vm_cloud(%s), available_slots(%s), shortfall_slots(%s,%s,%s), check the softmax or foreign VMs to try and see why not booting a new VM." % (
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("flavor"),
                                        config.categories['csmain']['max_start_vm_cloud'],
                                        available_slots,
                                        shortfall_cores,
                                        shortfall_disk,
                                        shortfall_ram
                                        ))
                                    continue

                                log.info("Trying to boot %s VMs (%s, %s); minumum of max_start_vm_cloud(%s), available_slots(%s), shortfall_slots(%s,%s,%s)." % (
                                    num_vms_to_boot,
                                    current_group_name,
                                    current_boot_flavor,
                                    config.categories['csmain']['max_start_vm_cloud'],
                                    available_slots,
                                    shortfall_cores,
                                    shortfall_disk,
                                    shortfall_ram
                                    ))
                                try:
                                    usertmp = idle_job.get("user").split('@')[0]
                                except:
                                    usertmp = idle_job.get("user")
                                try:
                                    cs_condor_host_ip = socket.gethostbyname(boot_flavor.get("htcondor_fqdn"))
                                except:
                                    cs_condor_host_ip = '*** unresolved **'
                                
                                if boot_flavor.get("worker_cert") is not None:
                                    if boot_flavor.get("worker_cert")[-1] == '\n':
                                        cloud_worker_cert = boot_flavor.get("worker_cert")[:-1].replace('\n', '\n        ')
                                    else:
                                        try:
                                            cloud_worker_cert = boot_flavor.get("worker_cert").decode().replace('\n', '\n        ')
                                        except AttributeError:
                                            cloud_worker_cert = boot_flavor.get("worker_cert").replace('\n', '\n        ')
                                else:
                                    cloud_worker_cert = None

                                if boot_flavor.get("worker_key") is not None:
                                    if boot_flavor.get("worker_key")[-1] == '\n':
                                        cloud_worker_key = boot_flavor.get("worker_key")[:-1].replace('\n', '\n        ')
                                    else:
                                        try:
                                            cloud_worker_key = boot_flavor.get("worker_key").decode().replace('\n', '\n        ')
                                        except AttributeError:
                                            cloud_worker_key = boot_flavor.get("worker_key").replace('\n', '\n        ')
                                else:
                                    cloud_worker_key = None

                                template_dict = {'cs_user': usertmp,
                                                 'cs_host': cs_host,
                                                 'cs_host_id': config.csv2_host_id,
                                                 'cs_host_ip': cs_host_ip,
                                                 'cs_group_name': boot_flavor.get("group_name"),
                                                 'cs_condor_host': boot_flavor.get("htcondor_fqdn"),
                                                 'cs_condor_host_ip': cs_condor_host_ip,
                                                 'cs_condor_name': boot_flavor.get("htcondor_container_hostname"),
                                                 'cs_condor_submitters': boot_flavor.get("htcondor_other_submitters"),
                                                 'cs_cloud_alias': idle_job.get("target_alias"),
                                                 'cs_condorworker_cert_days_to_end_of_life': config.categories['GSI']['cert_days_left_good'],
                                                 'cs_condorworker_optional_gsi_messages': config.categories['csmain']['condorworker_optional_gsi_msgs'],
                                                 'cs_condorworker_cert': cloud_worker_cert,
                                                 'cs_condorworker_key': cloud_worker_key}
                                ######## log.debug(template_dict)

                                # Let's try to boot
                                try:
                                    if idle_job.get("image"):
                                        use_image = idle_job.get("image")
                                    elif boot_flavor.get("default_image"):
                                        use_image = boot_flavor.get("default_image")
                                    else:
                                        use_image = None
                                    current_cloud_type = boot_flavor.get("cloud_type")
                                    if current_cloud_type == 'openstack':
                                        boot_cloud = openstackcloud.OpenStackCloud(config, resource=boot_flavor, metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        # check for volumes here?? make a new method in the openstack cloud object
                                        num_vms_to_boot = boot_cloud.check_volume_limits(num_vms_to_boot)
                                        if num_vms_to_boot <= 0:
                                            # not enough volume space
                                            continue
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)

                                    elif current_cloud_type == 'amazon':
                                        if idle_job.get("image"):
                                            use_image = ec2_image_dict[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")][idle_job.get("image")]
                                        elif boot_flavor.get("default_image"):
                                            use_image = ec2_image_dict[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")][boot_flavor.get("default_image")]
                                        boot_cloud = ec2cloud.EC2Cloud(config, resource=boot_flavor,
                                                 metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)


                                    elif current_cloud_type == 'localhost':
                                        boot_cloud = localhostcloud.LocalHostCloud(config, resource=boot_flavor,
                                                 metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)
                                    else:
                                        log.warning('Boot request for invalid cloud type "%s", ignoring.' % current_cloud_type)

                                    ####### log.debug('done booting on cloud %s', boot_flavor.get("cloud_name"))
                                    available_resources_update(used_resources, boot_flavor, num_vms_to_boot)
#                                   cloud_booted_on.add(boot_flavor.get("cloud_name"))
                                    booted_for_group = True
                                    break


                                except Exception as ex:
                                    log.exception("Disable cloud %s due to exception(later).", boot_flavor.get("cloud_name"))

                        if not booted_for_group:
                            log.info("No Flavor Slots for %s, target alias: %s." % (boot_flavor.get("group_name"), current_boot_target_alias))

        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
            time.sleep(2)
            continue
        config.db_close()
        while_counter += 1
        time.sleep(config.categories['csmain']['sleep_interval_main_long'])


def vm_termination(args_list):
    target_group = args_list[0]
    target_cloud = args_list[1]
    multiprocessing.current_process().name = "VM Termination"

    #over-ride signal handler defined in main
    signal.signal(signal.SIGTERM, _exit)

    # database setup
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', ['csmain', 'condor_poller.py',  "ProcessMonitor"], pool_size=3, signals=True)
    PID_FILE = config.categories["ProcessMonitor"]["pid_path"] + os.path.basename(sys.argv[0])

    cycle_start_time = 0
    new_poll_time = 0
    poll_time_history = [0,0,0,0]


    cycle_count = 0


    try:
        config.db_open()
        while True:
            logging.debug("%s : %s: Beginning vm retire cycle" % (target_group, target_cloud))
            config.refresh()

            if not os.path.exists(PID_FILE):
                logging.debug("%s : %s: Stop set, exiting..."  % (target_group, target_cloud))
                break

#            signal.signal(signal.SIGINT, signal.SIG_IGN)
            new_poll_time, cycle_start_time = start_cycle(new_poll_time, cycle_start_time)
       
            pair = {
                "group_name": target_group,
                "cloud_name": target_cloud,
            }
            process_group_cloud_terminates(pair, config)
            
            try:
                config.db_commit()
            except Exception as exc:
                logging.error("%s : %s: Error during final commit, likely that a vm was removed from database before final terminate update was comitted.."  % (target_group, target_cloud))
                logging.exception(exc)

            
#            signal.signal(signal.SIGINT, config.signals['SIGINT'])
            if not os.path.exists(PID_FILE):
                logging.info("%s : %s: Stop set, exiting..." % (target_group, target_cloud))
                break
            wait_cycle(cycle_start_time, poll_time_history, config.categories["condor_poller.py"]["sleep_interval_command"], config)

    except Exception as exc:
        logging.exception("Command consumer while loop exception, process terminating...")
        logging.error(exc)

#~~~~~~~~~~~~~~~~~

def remove_orphaned_volumes(config):
    CLOUD = "csv2_clouds"
    VOLUME = "cloud_volumes"
    sql_config = Config('/etc/cloudscheduler/cloudscheduler.yaml', [os.path.basename(sys.argv[0]), "SQL", "ProcessMonitor"], pool_size=3, signals=True)
    # config.db_open()
    rc, msg, volume_list = config.db_query(VOLUME)

    group_list = []
    for volume in volume_list:
        if volume.get("cloud_name") and volume.get("group_name"):
            group_list.append((volume["group_name"], volume["cloud_name"]))
    
    for volume in volume_list:
        volume_name = volume["name"]
        group_name = volume["group_name"] if volume.get("group_name") else ""
        cloud_name = volume["cloud_name"] if volume.get("cloud_name") else ""
        cloud_type = volume["cloud_type"] if volume.get("cloud_type") else ""
        if cloud_type == "openstack" and volume_name.startswith("vol-"):
            try:
                snip_vol_name = volume_name[4:] # remove the front "vol-"
                host_tokens = snip_vol_name.split("--")
                vol_group_name = host_tokens[0]
                vol_cloud_name = host_tokens[1] if len(host_tokens) > 1 else ''
                vol_host = int(host_tokens[2]) if len(host_tokens) > 2 else None
                if (vol_group_name, vol_cloud_name) not in group_list:
                    logging.debug("Group-Cloud combination doesn't match any in csv2, marking %s as foreign vol in cloud %s" % (volume_name, cloud_name))
                elif vol_host != int(sql_config.categories["SQL"]["csv2_host_id"]):
                    logging.debug("csv2 host id from host does not match (should be %s), marking %s as foreign vol in cloud %s" % (config.categories["SQL"]["csv2_host_id"], volume_name, cloud_name))
                else:
                    logging.debug("volume %s is a csv2 volume in cloud %s" % (volume_name, cloud_name))
                    volume_status = volume["status"]
                    if volume_status == 'error' or volume_status == 'available':
                        current_time = time.time()
                        created_time = volume["created_at"] if volume.get("created_at") else None
                        if created_time:
                            time_diff = (current_time - created_time) / 60 # current - created returns second / 60 returns mins
                        else:
                            logging.error("volume %s in cloud %s has no created time" % (volume_name, cloud_name))
                            continue
                        if time_diff > 15:
                            where_clause = "group_name='%s' and cloud_name='%s'" % (group_name, cloud_name)
                            rc, msg, clouds = config.db_query(CLOUD, where=where_clause)
                            cloud=clouds[0]
                            region = cloud["region"] if cloud.get("region") else None
                            verify = cloud["cacertificate"] if cloud.get("cacertificate") else None
                            sess = get_openstack_sess(cloud, verify)
                            if sess is False:
                                logging.error("Failed to establish session with %s, skipping this cloud..." % cloud_name)
                                continue
                            cinder = get_cinder_connection(sess, region)
                            if cinder is False:
                                logging.debug("Openstack cinder connection failed for %s, skipping this cloud..." % cloud_name)
                                continue
                            try:
                                cinder.delete_volume(volume["id"])
                                logging.info("Removed volume %s in cloud %s, as not attached to any server and live more than 15 mins" % (volume_name, cloud_name))
                                config.db_delete(VOLUME, volume)
                                config.db_commit()
                            except Exception as exc:
                                logging.error("Delete volume %s in cloud %s failed: %s" % (volume_name, cloud_name, exc))
                        else:
                            logging.debug("not attached to any server, volume %s in cloud %s created within 15 mins" % (volume_name, cloud_name))
            except Exception as exc:
                logging.error("Unable to get volume info: %s in cloud %s: %s" % (volume_name, cloud_name, exc))
        else:
            logging.debug("Not from openstack or doesn't match csv2 volume name pattern, marking %s as foreign vol in cloud %s" % (volume_name, cloud_name))


def disable_expired_clouds(config):
    # suppose database opened and will be closed in the main function
    CLOUD = "csv2_clouds"
    where_clause = "cloud_type='openstack'"
    #config.db_open()
    rc, msg, cloud_list = config.db_query(CLOUD, where=where_clause)
    try:
        for cloud in cloud_list:
            if cloud.get("app_credentials_expiry") and cloud.get("enabled") and cloud["enabled"] == 1:
                current_time = time.time()
                if (cloud["app_credentials_expiry"] - current_time) / 86400 < 8:
                    where_clause = "group_name='%s' and cloud_name='%s'" % (cloud['group_name'], cloud['cloud_name'])
                    cloud_row = { "enabled": 0 }
                    config.db_update(CLOUD, cloud_row, where=where_clause)
                    config.db_commit()
                    retire_cloud_vms(config, cloud["group_name"], cloud["cloud_name"])
        #config.db_close()
    except Exception as exc:
        logging.error("Unable to disable and retire the expired cloud: %s" % exc)
        #config.db_close()


if __name__ == '__main__':

    process_ids = {
        'scheduler': main,
        'idle_vms': check_view_idle_vms,
    #    'vm_termination': vm_termination,
        'vm_termination': [vm_termination, 'select distinct group_name,cloud_name from csv2_clouds;'],
    }

    procMon = ProcessMonitor(
        config_params=[os.path.basename(sys.argv[0]), "csmain", 'ProcessMonitor', 'general', 'condor_poller.py'],
        pool_size=15,
        process_ids=process_ids
        )

    config = procMon.get_config()
    log = procMon.get_logging()
    version = config.get_version()

    PID_FILE = config.categories["ProcessMonitor"]["pid_path"] + os.path.basename(sys.argv[0])
    with open(PID_FILE, "w") as fd:
        fd.write(str(os.getpid()))

    log.info("****************************"
             " starting CSv2 Scheduler processes - Running %s "
             "*********************************", version)

    # Wait for keyboard input to exit
    try:
        # start processes
        procMon.start_all()
        signal.signal(signal.SIGTERM, terminate)
        while True:
            config.refresh()
            config.update_service_catalog(logger=log)
            stop = check_pid(PID_FILE)
            procMon.check_processes(stop=stop)
            time.sleep(config.categories['ProcessMonitor']['sleep_interval_main_long'])

    except (SystemExit, KeyboardInterrupt):
        log.error("Caught KeyboardInterrupt, shutting down threads and exiting...")

    except Exception as ex:
        log.exception("Process Died: %s", ex)

    procMon.join_all()
